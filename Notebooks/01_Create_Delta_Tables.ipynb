
{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Spark session configuration\n",
                "This cell sets Spark session settings to enable _Verti-Parquet_ and _Optimize on Write_. More details about _Verti-Parquet_ and _Optimize on Write_ in tutorial document."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T17:53:12.0196844Z\",\"session_start_time\":\"2023-04-14T17:53:12.2618501Z\",\"execution_start_time\":\"2023-04-14T17:53:22.8437076Z\",\"execution_finish_time\":\"2023-04-14T17:53:24.8678387Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "# Copyright (c) Microsoft Corporation.\n",
                "# Licensed under the MIT License.\n",
                "\n",
                "spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
                "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n",
                "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Fact - Sale\n",
                "\n",
                "This cell reads raw data from the _Files_ section of the lakehouse, adds additional columns for different date parts and the same information is being used to create partitioned fact delta table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T18:02:25.4147613Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-04-14T18:02:25.768228Z\",\"execution_finish_time\":\"2023-04-14T18:03:21.4120056Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import col, year, month, quarter\n",
                "\n",
                "table_name = 'fact_sale'\n",
                "\n",
                "df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/fact_sale_1y_full')\n",
                "df = df.withColumn('Year', year(col(\"InvoiceDateKey\")))\n",
                "df = df.withColumn('Quarter', quarter(col(\"InvoiceDateKey\")))\n",
                "df = df.withColumn('Month', month(col(\"InvoiceDateKey\")))\n",
                "\n",
                "df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### Dimensions\n",
                "This cell creates a function to read raw data from the _Files_ section of the lakehouse for the table name passed as a parameter. Next, it creates a list of dimension tables. Finally, it has a _for loop_ to loop through the list of tables and call above function with each table name as parameter to read data for that specific table and create delta table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellStatus": "{\"Arshad Ali\":{\"queued_time\":\"2023-04-14T17:53:12.0270275Z\",\"session_start_time\":null,\"execution_start_time\":\"2023-04-14T17:56:22.4495972Z\",\"execution_finish_time\":\"2023-04-14T17:56:32.6772207Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\"}}",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.types import *\n",
                "\n",
                "def loadFullDataFromSource(table_name):\n",
                "    df = spark.read.format(\"parquet\").load('Files/wwi-raw-data/' + table_name)\n",
                "    df = df.select([c for c in df.columns if c != 'Photo'])\n",
                "    df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/\" + table_name)\n",
                "\n",
                "full_tables = [\n",
                "    'dimension_city',\n",
                "    'dimension_date',\n",
                "    'dimension_employee',\n",
                "    'dimension_stock_item'\n",
                "    ]\n",
                "\n",
                "for table in full_tables:\n",
                "    loadFullDataFromSource(table)"
            ]
        }
    ],
    "metadata": {
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "kernelspec": {
            "display_name": "Synapse PySpark",
            "name": "synapse_pyspark"
        },
        "language_info": {
            "name": "python"
        },
        "notebook_environment": {},
        "save_output": true,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {},
                "enableDebugMode": false
            }
        },
        "synapse_widget": {
            "state": {},
            "version": "0.1"
        },
        "trident": {
            "lakehouse": {}
        },
        "widgets": {}
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
